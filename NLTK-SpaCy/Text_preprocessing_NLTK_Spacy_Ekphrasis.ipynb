{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ondqQ43_TOLv"
      },
      "source": [
        "# Text preprocessing steps and universal pipeline\n",
        "\n",
        "Before feeding any ML model some kind data, it has to be properly preprocessed.\n",
        "\n",
        "Things we are going to discuss:\n",
        "\n",
        "1. Tokenization\n",
        "1. Cleaning\n",
        "1. Normalization\n",
        "1. Lemmatization\n",
        "1. Stemming\n",
        "\n",
        "Finally, we'll create reusable pipeline, which you'll be able to use in your applications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-Wg537kTOLy"
      },
      "source": [
        "example_text = \"\"\"\n",
        "An explosion targeting a tourist bus has injured at least 16 people near the\n",
        "Grand Egyptian Museum,\n",
        "next to the pyramids in Giza, security sources say E.U.\n",
        "\n",
        "South African tourists are among the injured. Most of those hurt suffered\n",
        "minor injuries,\n",
        "while three were treated in hospital, N.A.T.O. say.\n",
        "\n",
        "http://localhost:8888/notebooks/Text%20preprocessing.ipynb\n",
        "\n",
        "@nickname of twitter user and his email is email@gmail.com .\n",
        "\n",
        "A device went off close to the museum fence as the bus was passing on 16/02/2012.\n",
        "\"\"\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ3F8CVRTOL9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2839a7d-0c97-427c-a863-98ef52e84d06"
      },
      "source": [
        "#Requirements\n",
        "!pip install nltk\n",
        "!pip install spacy\n",
        "!pip install normalise\n",
        "#!pip install scikit-learn==0.23.2\n",
        "!pip install numpy\n",
        "\n",
        "import subprocess\n",
        "subprocess.run([\"python\", \"-m\", \"download\", \"en_core_web_sm\"])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "Collecting normalise\n",
            "  Downloading normalise-0.1.8-py3-none-any.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from normalise) (3.8.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from normalise) (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from normalise) (1.23.5)\n",
            "Collecting roman (from normalise)\n",
            "  Downloading roman-4.1-py3-none-any.whl (5.5 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from normalise) (1.11.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->normalise) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->normalise) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->normalise) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->normalise) (4.66.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->normalise) (3.2.0)\n",
            "Installing collected packages: roman, normalise\n",
            "Successfully installed normalise-0.1.8 roman-4.1\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['python', '-m', 'download', 'en_core_web_sm'], returncode=1)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJP5SvOXTOL-"
      },
      "source": [
        "# Tokenization\n",
        "\n",
        "`Tokenization` - text preprocessing step, which assumes splitting text into `tokens`(words, senteces, etc.)\n",
        "\n",
        "Seems like you can use somkeind of simple seperator to achieve it, but you don't have to forget that there are a lot of different situations, where separators just don't work. For example, `.` separator for tokenization into sentences will fail if you have abbreviations with dots. So you have to have more complex model to achieve good enough result. Commonly this problem is solved using `nltk` or `spacy` nlp libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZeEcjdUTOMW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf92498b-b9d5-4d99-f8c2-3b205c216941"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "nltk_words = word_tokenize(example_text)\n",
        "print(\"Tokenized words:\", nltk_words)\n",
        "\n",
        "nltk_sentences = sent_tokenize(example_text)\n",
        "print(\"Tokenized sentences:\", nltk_sentences)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized words: ['An', 'explosion', 'targeting', 'a', 'tourist', 'bus', 'has', 'injured', 'at', 'least', '16', 'people', 'near', 'the', 'Grand', 'Egyptian', 'Museum', ',', 'next', 'to', 'the', 'pyramids', 'in', 'Giza', ',', 'security', 'sources', 'say', 'E.U', '.', 'South', 'African', 'tourists', 'are', 'among', 'the', 'injured', '.', 'Most', 'of', 'those', 'hurt', 'suffered', 'minor', 'injuries', ',', 'while', 'three', 'were', 'treated', 'in', 'hospital', ',', 'N.A.T.O', '.', 'say', '.', 'http', ':', '//localhost:8888/notebooks/Text', '%', '20preprocessing.ipynb', '@', 'nickname', 'of', 'twitter', 'user', 'and', 'his', 'email', 'is', 'email', '@', 'gmail.com', '.', 'A', 'device', 'went', 'off', 'close', 'to', 'the', 'museum', 'fence', 'as', 'the', 'bus', 'was', 'passing', 'on', '16/02/2012', '.']\n",
            "Tokenized sentences: ['\\nAn explosion targeting a tourist bus has injured at least 16 people near the\\nGrand Egyptian Museum,\\nnext to the pyramids in Giza, security sources say E.U.', 'South African tourists are among the injured.', 'Most of those hurt suffered\\nminor injuries,\\nwhile three were treated in hospital, N.A.T.O.', 'say.', 'http://localhost:8888/notebooks/Text%20preprocessing.ipynb\\n\\n@nickname of twitter user and his email is email@gmail.com .', 'A device went off close to the museum fence as the bus was passing on 16/02/2012.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_ew58eATOMX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "026ce711-0fe7-4750-deee-1507061ea129"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(example_text)\n",
        "spacy_words = [token.text for token in doc]\n",
        "print(\"Tokenized words:\",spacy_words)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized words: ['\\n', 'An', 'explosion', 'targeting', 'a', 'tourist', 'bus', 'has', 'injured', 'at', 'least', '16', 'people', 'near', 'the', '\\n', 'Grand', 'Egyptian', 'Museum', ',', '\\n', 'next', 'to', 'the', 'pyramids', 'in', 'Giza', ',', 'security', 'sources', 'say', 'E.U.', '\\n\\n', 'South', 'African', 'tourists', 'are', 'among', 'the', 'injured', '.', 'Most', 'of', 'those', 'hurt', 'suffered', '\\n', 'minor', 'injuries', ',', '\\n', 'while', 'three', 'were', 'treated', 'in', 'hospital', ',', 'N.A.T.O.', 'say', '.', '\\n\\n', 'http://localhost:8888', '/', 'notebooks', '/', 'Text%20preprocessing.ipynb', '\\n\\n', '@nickname', 'of', 'twitter', 'user', 'and', 'his', 'email', 'is', 'email@gmail.com', '.', '\\n\\n', 'A', 'device', 'went', 'off', 'close', 'to', 'the', 'museum', 'fence', 'as', 'the', 'bus', 'was', 'passing', 'on', '16/02/2012', '.', '\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "junuUjjZTOMX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e293ee5-7e30-46de-f3c3-cd98cd0de833"
      },
      "source": [
        "print(\"In spacy but not in nltk:\",set(spacy_words).difference(set(nltk_words)))\n",
        "\n",
        "print(\"In nltk but not in spacy:\", set(nltk_words).difference(set(spacy_words)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In spacy but not in nltk: {'http://localhost:8888', 'email@gmail.com', '\\n', '@nickname', 'E.U.', '\\n\\n', 'N.A.T.O.', 'notebooks', '/', 'Text%20preprocessing.ipynb'}\n",
            "In nltk but not in spacy: {'20preprocessing.ipynb', 'gmail.com', ':', '@', 'http', 'E.U', '//localhost:8888/notebooks/Text', 'N.A.T.O', 'nickname', '%'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LsQnc-STOMY"
      },
      "source": [
        "We see that `spacy` tokenized some weird staff like `\\n`, `\\n\\n`, but was able to handle urls, emails and twitter-like mentions. Also we see that `nltk` tokenized abbreviations without the last `.`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tzKxaj0TOMb"
      },
      "source": [
        "# Cleaning\n",
        "\n",
        "`Cleaning` step assumes removing all undesirable content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0jSH2QiTOMb"
      },
      "source": [
        "### Punctuation removal\n",
        "`Punctuation removal` might be a good step, when punctuation does not brings additional value for text vectorization. Punctuation removal is better to be done after tokenization step, doing it before might cause undesirable effects. Good choice for `TF-IDF`, `Count`, `Binary` vectorization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPgDMdMBTOMc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3a202de-f513-440a-cac3-54b7e3d88a23"
      },
      "source": [
        "import string\n",
        "print(\"Punctuation symbols:\",string.punctuation)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Punctuation symbols: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFPAZTktTOMd"
      },
      "source": [
        "text_with_punct = \"@nickname of twitter user, and his email is email@gmail.com.\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNEXCVEKTOMd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67e0258d-ddfb-4cb1-a6b8-0c9843fd68d5"
      },
      "source": [
        "text_without_punct = text_with_punct.translate(str.maketrans('', '', string.punctuation))\n",
        "print(\"Text without punctuation:\",text_without_punct)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text without punctuation: nickname of twitter user and his email is emailgmailcom\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UAecY4TTOMe"
      },
      "source": [
        "Here you can see that important symbols for correct tokenizations were removed. Now email can't be properly detected. As you could mention from the `Tokenization` step, punctuation symbors were parsed as single tokens, so better way would be to tokenize first and then remove punctuation symbols."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nywThd_qTOMe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fccdf18b-dd13-4e9f-81d7-823a02d922ad"
      },
      "source": [
        "doc = nlp(text_with_punct)\n",
        "tokens = [t.text for t in doc]\n",
        "\n",
        "# string\n",
        "tokens_without_punct_python = [t for t in tokens if t not in string.punctuation]\n",
        "print(\"Python based removal:\",tokens_without_punct_python)\n",
        "\n",
        "# spacy\n",
        "tokens_without_punct_spacy = [t.text for t in doc if t.pos_ != 'PUNCT']\n",
        "print(\"Spacy based removal:\", tokens_without_punct_spacy)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python based removal: ['@nickname', 'of', 'twitter', 'user', 'and', 'his', 'email', 'is', 'email@gmail.com']\n",
            "Spacy based removal: ['@nickname', 'of', 'twitter', 'user', 'and', 'his', 'email', 'is', 'email@gmail.com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM3t_qAvTOMf"
      },
      "source": [
        "### Stop words removal\n",
        "\n",
        "`Stop words` usually refers to the most common words in a language, which usualy does not bring additional meaning. There is no single universal list of stop words used by all nlp tools, because this term has very fuzzy definition. Although practice has shown, that this step is much have, when preparing text for indexing, but might be tricky for text classification purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ56N7uZTOMf"
      },
      "source": [
        "text = \"This movie is just not good enough\""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vPfCbbcTOMf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9912355-9dd6-45b5-a052-75259df627a2"
      },
      "source": [
        "#spacy\n",
        "spacy_stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "print(\"Spacy stop words count:\", len(spacy_stop_words))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spacy stop words count: 326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AywZyJI3TOMf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e21c301-f91d-473c-8105-dd08241fd9b7"
      },
      "source": [
        "text_without_stop_words = [t.text for t in nlp(text) if not t.is_stop]\n",
        "print(\"Spacy text without stop words:\", text_without_stop_words)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spacy text without stop words: ['movie', 'good']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmAxDryaTOMg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22ef3bb3-d20a-4667-b979-542dfcca12f8"
      },
      "source": [
        "#nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nltk_stop_words = nltk.corpus.stopwords.words('english')\n",
        "print(\"nltk stop words count:\", len(nltk_stop_words))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nltk stop words count: 179\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWMdECZtTOMg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d65bab3-7902-4186-a87f-7a62ec6e6047"
      },
      "source": [
        "text_without_stop_words = [t for t in word_tokenize(text)\n",
        "  if t not in nltk_stop_words]\n",
        "print(\"nltk text without stop words:\", text_without_stop_words)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nltk text without stop words: ['This', 'movie', 'good', 'enough']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAbKOzD6TOMg"
      },
      "source": [
        "Here you see that nltk and spacy has different vocabulary size, so the results of filtering are different. But the main thing I want to underline is that the word `not` was filtered, which in the most cases will be allright, but in the case when you want determine the polarity of this sentence `not` will bring the additional meaning.\n",
        "\n",
        "For such cases you are able to set stop words you can ignore in spacy library. In the case of nltk you cat just remove or add custom words to `nltk_stop_words`, it is just a list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5jpPVYiTOMg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2c9c13a-45af-4c4e-e465-ad35b2d1c5e5"
      },
      "source": [
        "import en_core_web_sm\n",
        "\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "customize_stop_words = [\n",
        "    'not'\n",
        "]\n",
        "\n",
        "for w in customize_stop_words:\n",
        "    nlp.vocab[w].is_stop = False\n",
        "\n",
        "text_without_stop_words = [t.text for t in nlp(text) if not t.is_stop]\n",
        "print(\"Spacy text without updated stop words:\", text_without_stop_words)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spacy text without updated stop words: ['movie', 'not', 'good']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YcS-yoITOMh"
      },
      "source": [
        "# Normalization\n",
        "\n",
        "Like any data text requires normalization. In case of text it is:\n",
        "\n",
        "1. Converting dates to text\n",
        "2. Numbers to text\n",
        "3. Currency/Percent signs to text\n",
        "4. Spelling mistakes correction\n",
        "\n",
        "To summarize, normalization is a convertion of any non-text information into textual equivalent.\n",
        "\n",
        "For this purposes exists a great library - [NVIDIA/NeMo-text-processing](https://github.com/NVIDIA/NeMo-text-processing)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pynini==2.1.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0VaFAevGDV9",
        "outputId": "16c5ee86-8084-4e6c-acaf-7271bf95df14"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pynini==2.1.5\n",
            "  Downloading pynini-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.3/161.3 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Cython>=0.29 in /usr/local/lib/python3.10/dist-packages (from pynini==2.1.5) (3.0.5)\n",
            "Installing collected packages: pynini\n",
            "Successfully installed pynini-2.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bgoaKnXtFY0s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30c9adad-c314-43ac-cb5b-af276d7f57a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nemo_text_processing\n",
            "  Cloning https://github.com/NVIDIA/NeMo-text-processing.git (to revision main) to /tmp/pip-install-685er9fs/nemo-text-processing_1ff24392297d4f8aa2f404fc30fb725d\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/NVIDIA/NeMo-text-processing.git /tmp/pip-install-685er9fs/nemo-text-processing_1ff24392297d4f8aa2f404fc30fb725d\n",
            "  Resolved https://github.com/NVIDIA/NeMo-text-processing.git to commit 5dd753a8807b3b3bd9aea954776b71bd73fdb870\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cdifflib (from nemo_text_processing)\n",
            "  Downloading cdifflib-1.2.6.tar.gz (11 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from nemo_text_processing) (0.6.2)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from nemo_text_processing) (7.0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nemo_text_processing) (1.3.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from nemo_text_processing) (1.5.3)\n",
            "Requirement already satisfied: pynini==2.1.5 in /usr/local/lib/python3.10/dist-packages (from nemo_text_processing) (2.1.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from nemo_text_processing) (2023.6.3)\n",
            "Collecting sacremoses>=0.0.43 (from nemo_text_processing)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=65.5.1 in /usr/local/lib/python3.10/dist-packages (from nemo_text_processing) (67.7.2)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from nemo_text_processing) (4.66.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from nemo_text_processing) (4.35.2)\n",
            "Collecting wget (from nemo_text_processing)\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from nemo_text_processing) (1.14.1)\n",
            "Requirement already satisfied: Cython>=0.29 in /usr/local/lib/python3.10/dist-packages (from pynini==2.1.5->nemo_text_processing) (3.0.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses>=0.0.43->nemo_text_processing) (8.1.7)\n",
            "Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from inflect->nemo_text_processing) (1.10.13)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from inflect->nemo_text_processing) (4.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->nemo_text_processing) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->nemo_text_processing) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->nemo_text_processing) (1.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->nemo_text_processing) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers->nemo_text_processing) (0.19.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->nemo_text_processing) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->nemo_text_processing) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->nemo_text_processing) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->nemo_text_processing) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->nemo_text_processing) (0.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers->nemo_text_processing) (2023.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->nemo_text_processing) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->nemo_text_processing) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->nemo_text_processing) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->nemo_text_processing) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->nemo_text_processing) (2023.7.22)\n",
            "Building wheels for collected packages: nemo_text_processing, cdifflib, wget\n",
            "  Building wheel for nemo_text_processing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nemo_text_processing: filename=nemo_text_processing-0.2.2rc0-py3-none-any.whl size=2474744 sha256=1eafda21842be3d0c958f083ea2a1b6bbc67c3277972e09c7ed221cf05f49b21\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-erayrz3a/wheels/9c/11/56/c857887d98ffcb7eaa21806a6eeae17d68369b58d753179445\n",
            "  Building wheel for cdifflib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cdifflib: filename=cdifflib-1.2.6-cp310-cp310-linux_x86_64.whl size=27680 sha256=a0ff08c2a3ad7bd8df8db2931807be13931f07731f4401e0bf9255195350c428\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/a7/fd/8061e24ed08689045cb6d1ca303768dc463b20a5a338174841\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9657 sha256=9b4e345e661eb1fc81b8779a929f2dfbeef819657f19d32b55daa1bf3d99b153\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built nemo_text_processing cdifflib wget\n",
            "Installing collected packages: wget, sacremoses, cdifflib, nemo_text_processing\n",
            "Successfully installed cdifflib-1.2.6 nemo_text_processing-0.2.2rc0 sacremoses-0.1.1 wget-3.2\n"
          ]
        }
      ],
      "source": [
        "## Install NeMo-text-processing\n",
        "BRANCH = 'main'\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo-text-processing.git@$BRANCH#egg=nemo_text_processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ut8MqVNWFY0s"
      },
      "outputs": [],
      "source": [
        "# try to import of nemo_text_processing an other dependencies\n",
        "import nemo_text_processing\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Bfs7fa9lXDDh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e07b684-a187-4e58-a7ba-484ee5414781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " NeMo-text-processing :: INFO     :: Creating ClassifyFst grammars.\n",
            "INFO:NeMo-text-processing:Creating ClassifyFst grammars.\n"
          ]
        }
      ],
      "source": [
        "# create text normalization instance that works on cased input\n",
        "from nemo_text_processing.text_normalization.normalize import Normalizer\n",
        "normalizer = Normalizer(input_case='lower_cased', lang='en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8Gxex8A_FY0t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d792949e-1338-4086-e14a-57c8402082eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " NeMo-text-processing :: DEBUG    :: tokens { name: \"on\" } tokens { date { day: \"thirteen\" month: \"february\" year: \"two thousand seven\" preserve_order: true } }  tokens { name: \",\" } tokens { name: \"theresa\" } tokens { name: \"may\" } tokens { name: \"announced\" } tokens { name: \"on\" } tokens { name: \"mtv\" } tokens { name: \"news\" } tokens { name: \"that\" } tokens { name: \"the\" } tokens { name: \"rate\" } tokens { name: \"of\" } tokens { name: \"childhod\" } tokens { name: \"obesity\" } tokens { name: \"had\" } tokens { name: \"risen\" } tokens { name: \"from\" } tokens { decimal { integer_part: \"seven\"  fractional_part: \"three\" } }  tokens { name: \"-\" } tokens { measure { decimal { integer_part: \"nine\"  fractional_part: \"six\" } units: \"percent\" } } tokens { name: \"in\" } tokens { name: \"just\" } tokens { cardinal { integer: \"three\" } } tokens { name: \"years\" }  tokens { name: \",\" }  tokens { name: \"costing\" } tokens { name: \"the\" } tokens { name: \"n.a.t.o\" } tokens { name: \"pound twenty m\" }\n",
            "DEBUG:NeMo-text-processing:tokens { name: \"on\" } tokens { date { day: \"thirteen\" month: \"february\" year: \"two thousand seven\" preserve_order: true } }  tokens { name: \",\" } tokens { name: \"theresa\" } tokens { name: \"may\" } tokens { name: \"announced\" } tokens { name: \"on\" } tokens { name: \"mtv\" } tokens { name: \"news\" } tokens { name: \"that\" } tokens { name: \"the\" } tokens { name: \"rate\" } tokens { name: \"of\" } tokens { name: \"childhod\" } tokens { name: \"obesity\" } tokens { name: \"had\" } tokens { name: \"risen\" } tokens { name: \"from\" } tokens { decimal { integer_part: \"seven\"  fractional_part: \"three\" } }  tokens { name: \"-\" } tokens { measure { decimal { integer_part: \"nine\"  fractional_part: \"six\" } units: \"percent\" } } tokens { name: \"in\" } tokens { name: \"just\" } tokens { cardinal { integer: \"three\" } } tokens { name: \"years\" }  tokens { name: \",\" }  tokens { name: \"costing\" } tokens { name: \"the\" } tokens { name: \"n.a.t.o\" } tokens { name: \"pound twenty m\" }\n",
            " NeMo-text-processing :: INFO     :: Skipping post-processing of on the thirteenth of february two thousand seven, theresa may announced on mtv news that the rate of childhod obesity had risen from seven point three - nine point six percent in just three years, costing the n.a.t.o pound twenty m for '%'\n",
            "INFO:NeMo-text-processing:Skipping post-processing of on the thirteenth of february two thousand seven, theresa may announced on mtv news that the rate of childhod obesity had risen from seven point three - nine point six percent in just three years, costing the n.a.t.o pound twenty m for '%'\n"
          ]
        }
      ],
      "source": [
        "# run normalization on example string input\n",
        "\n",
        "text = \"\"\"\n",
        "On the 13 Feb. 2007, Theresa May announced on MTV news that the\n",
        "rate of childhod obesity had\n",
        "risen from 7.3-9.6% in just 3 years , costing the N.A.T.O £20m\n",
        "\"\"\"\n",
        "text = text.lower()\n",
        "normalized = normalizer.normalize(text, verbose=True, punct_post_process=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(normalized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HF07SkefGRC_",
        "outputId": "f5448dc1-c46f-4414-8a6b-80090a50ccad"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "on the thirteenth of february two thousand seven, theresa may announced on mtv news that the rate of childhod obesity had risen from seven point three-nine point six percent in just three years , costing the n.a.t.o pound twenty m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JuH-vqwTOMk"
      },
      "source": [
        "# Lemmatization and Stemming\n",
        "\n",
        "`Stemming` is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language.\n",
        "\n",
        "`Lemmatization`, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH3G8cJiTOMk"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "import numpy as np\n",
        "\n",
        "tokens = word_tokenize(text)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBsk7sIqTOMk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41a65c7b-02de-45f6-cca7-a8d04493d31b"
      },
      "source": [
        "porter=PorterStemmer()\n",
        "stem_words = np.vectorize(porter.stem)\n",
        "stemed_text = ' '.join(stem_words(tokens))\n",
        "print(\"Stemed text:\", stemed_text)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemed text: on the 13 feb. 2007 , theresa may announc on mtv news that the rate of childhod obes had risen from 7.3-9.6 % in just 3 year , cost the n.a.t.o £20m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkZaHF7yTOMk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6adeef9-de26-4612-bec9-33577ae80717"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "lemmatize_words = np.vectorize(wordnet_lemmatizer.lemmatize)\n",
        "lemmatized_text = ' '.join(lemmatize_words(tokens))\n",
        "print(\"nltk lemmatized text:\",lemmatized_text)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nltk lemmatized text: on the 13 feb. 2007 , theresa may announced on mtv news that the rate of childhod obesity had risen from 7.3-9.6 % in just 3 year , costing the n.a.t.o £20m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDByjZImTOMl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8099af00-8fbb-46fd-b51f-01cf51f6fd51"
      },
      "source": [
        "lemmas = [t.lemma_ for t in nlp(text)]\n",
        "print(\"Spacy lemmatized text:\",(' '.join(lemmas)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spacy lemmatized text: \n",
            " on the 13 feb . 2007 , theresa may announce on mtv news that the \n",
            " rate of childhod obesity have \n",
            " rise from 7.3 - 9.6 % in just 3 year , cost the n.a.t.o £ 20 m \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyyR9UAKTOMl"
      },
      "source": [
        "We see that `spacy` lemmatized much better than nltk, one of examples `risen` -> `rise`, only `spacy` handeled that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKTfHG4BTOMl"
      },
      "source": [
        "# Reusable pipeline\n",
        "\n",
        "And now my favourite part! We are going to cretate reusable pipeline, which you could use on any of you projects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHpgTyLPTOMl"
      },
      "source": [
        "import numpy as np\n",
        "import multiprocessing as mp\n",
        "import copy\n",
        "import string\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "from nemo_text_processing.text_normalization.normalize import Normalizer\n",
        "\n",
        "\n",
        "nlp = en_core_web_sm.load()\n",
        "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Text preprocessing transformer includes steps:\n",
        "            1. Text normalization\n",
        "            2. Punctuation removal\n",
        "            3. Stop words removal\n",
        "            4. Lemmatization\n",
        "        \"\"\"\n",
        "        self.normalizer = Normalizer(input_case='lower_cased', lang='en')\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = copy.copy(X)\n",
        "        return self._preprocess_text(X_copy)\n",
        "\n",
        "    def _preprocess_part(self, part):\n",
        "        return part.apply(self._preprocess_text)\n",
        "\n",
        "    def _preprocess_text(self, text):\n",
        "        normalized_text = self._normalize(text)\n",
        "        doc = nlp(normalized_text)\n",
        "        removed_punct = self._remove_punct(doc)\n",
        "        removed_stop_words = self._remove_stop_words(removed_punct)\n",
        "        return self._lemmatize(removed_stop_words)\n",
        "\n",
        "    def _normalize(self, text):\n",
        "        try:\n",
        "            norm = self.normalizer.normalize(text, verbose=False, punct_post_process=True, punct_pre_process=True)\n",
        "            return norm\n",
        "        except:\n",
        "            return text\n",
        "\n",
        "    def _remove_punct(self, doc):\n",
        "        return [t for t in doc if t.text not in string.punctuation]\n",
        "\n",
        "    def _remove_stop_words(self, doc):\n",
        "        return [t for t in doc if not t.is_stop]\n",
        "\n",
        "    def _lemmatize(self, doc):\n",
        "        return ' '.join([t.lemma_ for t in doc])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "lYv7F7ddTOMm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3ba19c9-5c22-42eb-f272-7ccaba37fc75"
      },
      "source": [
        "%%time\n",
        "text = TextPreprocessor().transform(example_text)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " NeMo-text-processing :: INFO     :: Creating ClassifyFst grammars.\n",
            "INFO:NeMo-text-processing:Creating ClassifyFst grammars.\n",
            " NeMo-text-processing :: DEBUG    :: cardinal:  1.24s -- 6247 nodes\n",
            "DEBUG:NeMo-text-processing:cardinal:  1.24s -- 6247 nodes\n",
            " NeMo-text-processing :: DEBUG    :: ordinal:  1.56s -- 1478 nodes\n",
            "DEBUG:NeMo-text-processing:ordinal:  1.56s -- 1478 nodes\n",
            " NeMo-text-processing :: DEBUG    :: decimal:  0.60s -- 3151 nodes\n",
            "DEBUG:NeMo-text-processing:decimal:  0.60s -- 3151 nodes\n",
            " NeMo-text-processing :: DEBUG    :: fraction:  0.88s -- 4254 nodes\n",
            "DEBUG:NeMo-text-processing:fraction:  0.88s -- 4254 nodes\n",
            " NeMo-text-processing :: DEBUG    :: measure:  12.93s -- 49430 nodes\n",
            "DEBUG:NeMo-text-processing:measure:  12.93s -- 49430 nodes\n",
            " NeMo-text-processing :: DEBUG    :: date:  0.62s -- 4456 nodes\n",
            "DEBUG:NeMo-text-processing:date:  0.62s -- 4456 nodes\n",
            " NeMo-text-processing :: DEBUG    :: time:  0.17s -- 418 nodes\n",
            "DEBUG:NeMo-text-processing:time:  0.17s -- 418 nodes\n",
            " NeMo-text-processing :: DEBUG    :: telephone:  0.65s -- 3467 nodes\n",
            "DEBUG:NeMo-text-processing:telephone:  0.65s -- 3467 nodes\n",
            " NeMo-text-processing :: DEBUG    :: electronic:  0.16s -- 902 nodes\n",
            "DEBUG:NeMo-text-processing:electronic:  0.16s -- 902 nodes\n",
            " NeMo-text-processing :: DEBUG    :: money:  11.10s -- 13153 nodes\n",
            "DEBUG:NeMo-text-processing:money:  11.10s -- 13153 nodes\n",
            " NeMo-text-processing :: DEBUG    :: whitelist:  1.25s -- 16688 nodes\n",
            "DEBUG:NeMo-text-processing:whitelist:  1.25s -- 16688 nodes\n",
            " NeMo-text-processing :: DEBUG    :: punct:  2.60s -- 259 nodes\n",
            "DEBUG:NeMo-text-processing:punct:  2.60s -- 259 nodes\n",
            " NeMo-text-processing :: DEBUG    :: word:  7.33s -- 1295 nodes\n",
            "DEBUG:NeMo-text-processing:word:  7.33s -- 1295 nodes\n",
            " NeMo-text-processing :: DEBUG    :: serial:  8.19s -- 10772 nodes\n",
            "DEBUG:NeMo-text-processing:serial:  8.19s -- 10772 nodes\n",
            " NeMo-text-processing :: DEBUG    :: range:  2.04s -- 14116 nodes\n",
            "DEBUG:NeMo-text-processing:range:  2.04s -- 14116 nodes\n",
            " NeMo-text-processing :: INFO     :: Skipping post-processing of An explosion targeting a tourist bus has injured at least sixteen people near the Grand Egyptian Museum, next to the pyramids in Giza, security sources say E.U. South African tourists are among the injured. Most of those hurt suffered minor injuries, while three were treated in hospital, NATO say. HTTP colon slash slash localhost: eight thousand eight hundred eighty eight slash notebooks slash Text percent twenty preprocessing. ipynb at nickname of twitter user and his email is email at gmail dot com. A device went off close to the museum fence as the bus was passing on the sixteenth of february twenty twelve. for '%'\n",
            "INFO:NeMo-text-processing:Skipping post-processing of An explosion targeting a tourist bus has injured at least sixteen people near the Grand Egyptian Museum, next to the pyramids in Giza, security sources say E.U. South African tourists are among the injured. Most of those hurt suffered minor injuries, while three were treated in hospital, NATO say. HTTP colon slash slash localhost: eight thousand eight hundred eighty eight slash notebooks slash Text percent twenty preprocessing. ipynb at nickname of twitter user and his email is email at gmail dot com. A device went off close to the museum fence as the bus was passing on the sixteenth of february twenty twelve. for '%'\n",
            " NeMo-text-processing :: INFO     :: Skipping post-processing of An explosion targeting a tourist bus has injured at least sixteen people near the Grand Egyptian Museum,next to the pyramids in Giza, security sources say E.U.South African tourists are among the injured. Most of those hurt suffered minor injuries,while three were treated in hospital, NATO say. HTTP colon slash slash localhost: eight thousand eight hundred eighty eight slash notebooks slash Text percent twenty preprocessing.ipynb at nickname of twitter user and his email is email at gmail dot com. A device went off close to the museum fence as the bus was passing on the sixteenth of february twenty twelve. for '/'\n",
            "INFO:NeMo-text-processing:Skipping post-processing of An explosion targeting a tourist bus has injured at least sixteen people near the Grand Egyptian Museum,next to the pyramids in Giza, security sources say E.U.South African tourists are among the injured. Most of those hurt suffered minor injuries,while three were treated in hospital, NATO say. HTTP colon slash slash localhost: eight thousand eight hundred eighty eight slash notebooks slash Text percent twenty preprocessing.ipynb at nickname of twitter user and his email is email at gmail dot com. A device went off close to the museum fence as the bus was passing on the sixteenth of february twenty twelve. for '/'\n",
            " NeMo-text-processing :: INFO     :: Skipping post-processing of An explosion targeting a tourist bus has injured at least sixteen people near the Grand Egyptian Museum,next to the pyramids in Giza, security sources say E.U.South African tourists are among the injured. Most of those hurt suffered minor injuries,while three were treated in hospital, NATO say. HTTP colon slash slash localhost:eight thousand eight hundred eighty eight slash notebooks slash Text percent twenty preprocessing.ipynb at nickname of twitter user and his email is email at gmail dot com. A device went off close to the museum fence as the bus was passing on the sixteenth of february twenty twelve. for '@'\n",
            "INFO:NeMo-text-processing:Skipping post-processing of An explosion targeting a tourist bus has injured at least sixteen people near the Grand Egyptian Museum,next to the pyramids in Giza, security sources say E.U.South African tourists are among the injured. Most of those hurt suffered minor injuries,while three were treated in hospital, NATO say. HTTP colon slash slash localhost:eight thousand eight hundred eighty eight slash notebooks slash Text percent twenty preprocessing.ipynb at nickname of twitter user and his email is email at gmail dot com. A device went off close to the museum fence as the bus was passing on the sixteenth of february twenty twelve. for '@'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 3s, sys: 462 ms, total: 1min 4s\n",
            "Wall time: 1min 10s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v1XElX3J_3t",
        "outputId": "07f98b16-9136-4d93-c799-58e6330ceb68"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "explosion target tourist bus injure sixteen people near Grand Egyptian Museum pyramid Giza security source e.u.south african tourist injure hurt suffer minor injury treat hospital NATO HTTP colon slash slash localhost thousand eighty slash notebook slash Text percent preprocessing.ipynb nickname twitter user email email gmail dot com device go close museum fence bus pass sixteenth february\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxUMzD6MKnnI"
      },
      "source": [
        "#Ekphrasis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS8G1vDLL3kp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d54c6b3f-0a8c-4c41-8239-0626842644a9"
      },
      "source": [
        "!pip install ekphrasis"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ekphrasis\n",
            "  Downloading ekphrasis-0.5.4-py3-none-any.whl (83 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/83.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m81.9/83.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.8/83.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (2.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (4.66.1)\n",
            "Collecting colorama (from ekphrasis)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting ujson (from ekphrasis)\n",
            "  Downloading ujson-5.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (3.7.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (3.8.1)\n",
            "Collecting ftfy (from ekphrasis)\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (1.23.5)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->ekphrasis) (0.2.10)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (2.8.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis) (2023.6.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->ekphrasis) (1.16.0)\n",
            "Installing collected packages: ujson, ftfy, colorama, ekphrasis\n",
            "Successfully installed colorama-0.4.6 ekphrasis-0.5.4 ftfy-6.1.1 ujson-5.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "128Mr2rRKpwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "180b9957-9094-49b0-aa28-53828742b36c"
      },
      "source": [
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    # terms that will be normalized\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    # terms that will be annotated\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    fix_html=True,  # fix HTML tokens\n",
        "\n",
        "    # corpus from which the word statistics are going to be used\n",
        "    # for word segmentation\n",
        "    segmenter=\"twitter\",\n",
        "\n",
        "    # corpus from which the word statistics are going to be used\n",
        "    # for spell correction\n",
        "    corrector=\"twitter\",\n",
        "\n",
        "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
        "    spell_correct_elong=False,  # spell correction for elongated words\n",
        "\n",
        "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "    # the tokenizer, should take as input a string and return a list of tokens\n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "\n",
        "    # list of dictionaries, for replacing tokens extracted from the text,\n",
        "    # with other expressions. You can pass more than one dictionaries.\n",
        "    dicts=[emoticons]\n",
        ")\n",
        "\n",
        "sentences = [\n",
        "    \"CANT WAIT for the new season of #TwinPeaks ＼(^o^)／!!! #davidlynch #tvseries :)))\",\n",
        "    \"I saw the new #johndoe movie and it suuuuucks!!! WAISTED $10... #badmovies :/\",\n",
        "    \"@SentimentSymp:  can't wait for the Nov 9 #Sentiment talks!  YAAAAAAY !!! :-D http://sentimentsymposium.com/.\"\n",
        "]\n",
        "for s in sentences:\n",
        "    print(\" \".join(text_processor.pre_process_doc(s)))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
            "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word statistics files not found!\n",
            "Downloading... done!\n",
            "Unpacking... done!\n",
            "Reading twitter - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_1grams.txt\n",
            "Reading twitter - 2grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_2grams.txt\n",
            "Reading twitter - 1grams ...\n",
            "<allcaps> cant wait </allcaps> for the new season of <hashtag> twin peaks </hashtag> ＼(^o^)／ ! <repeated> <hashtag> david lynch </hashtag> <hashtag> tv series </hashtag> <happy>\n",
            "i saw the new <hashtag> john doe </hashtag> movie and it sucks <elongated> ! <repeated> <allcaps> waisted </allcaps> <money> . <repeated> <hashtag> bad movies </hashtag> <annoyed>\n",
            "<user> : can not wait for the <date> <hashtag> sentiment </hashtag> talks ! <allcaps> yay <elongated> </allcaps> ! <repeated> <laugh> <url>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D5joBTYNEXu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0b9ea87-0d5a-40ce-d584-89e4aa8d3657"
      },
      "source": [
        "text_processor.pre_process_doc(sentences[1])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'saw',\n",
              " 'the',\n",
              " 'new',\n",
              " '<hashtag>',\n",
              " 'john',\n",
              " 'doe',\n",
              " '</hashtag>',\n",
              " 'movie',\n",
              " 'and',\n",
              " 'it',\n",
              " 'sucks',\n",
              " '<elongated>',\n",
              " '!',\n",
              " '<repeated>',\n",
              " '<allcaps>',\n",
              " 'waisted',\n",
              " '</allcaps>',\n",
              " '<money>',\n",
              " '.',\n",
              " '<repeated>',\n",
              " '<hashtag>',\n",
              " 'bad',\n",
              " 'movies',\n",
              " '</hashtag>',\n",
              " '<annoyed>']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R98GyggIMaWX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50edb7f5-5b2f-463d-eeab-988c031be570"
      },
      "source": [
        "for s in sentences:\n",
        "    print(\" \".join(text_processor.pre_process_doc(s)))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<allcaps> cant wait </allcaps> for the new season of <hashtag> twin peaks </hashtag> ＼(^o^)／ ! <repeated> <hashtag> david lynch </hashtag> <hashtag> tv series </hashtag> <happy>\n",
            "i saw the new <hashtag> john doe </hashtag> movie and it sucks <elongated> ! <repeated> <allcaps> waisted </allcaps> <money> . <repeated> <hashtag> bad movies </hashtag> <annoyed>\n",
            "<user> : can not wait for the <date> <hashtag> sentiment </hashtag> talks ! <allcaps> yay <elongated> </allcaps> ! <repeated> <laugh> <url>\n"
          ]
        }
      ]
    }
  ]
}
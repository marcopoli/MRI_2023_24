{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsxP6VdezGId",
        "outputId": "db6dec5d-007b-49e9-dac7-9c2b835ce475",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kAaBewyzGIm"
      },
      "source": [
        "Create a Dictionary from a list of sentences\n",
        "======================================"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYLcngJEzGIo"
      },
      "source": [
        "In gensim, the dictionary contains a map of all words (tokens) to its unique id.\n",
        "\n",
        "You can create a dictionary from a paragraph of sentences, from a text file that contains multiple lines of text and from multiple such text files contained in a directory. For the second and third cases, we will do it without loading the entire file into memory so that the dictionary gets updated as you read the text line by line.\n",
        "\n",
        "Let’s start with the ‘List of sentences’ input.\n",
        "\n",
        "When you have multiple sentences, you need to convert each sentence to a list of words. List comprehensions is a common way to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4MNp2PJzGIp",
        "outputId": "2955f0ad-c493-4017-da3b-0fd5100fc49e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "# How to create a dictionary from a list of sentences?\n",
        "documents = [\"The Saudis are preparing a report that will acknowledge that\",\n",
        "             \"Saudi journalist Jamal Khashoggi's death was the result of an\",\n",
        "             \"interrogation that went wrong, one that was intended to lead\",\n",
        "             \"to his abduction from Turkey, according to two sources.\"]\n",
        "\n",
        "documents_2 = [\"One source says the report will likely conclude that\",\n",
        "                \"the operation was carried out without clearance and\",\n",
        "                \"transparency and that those involved will be held\",\n",
        "                \"responsible. One of the sources acknowledged that the\",\n",
        "                \"report is still being prepared and cautioned that\",\n",
        "                \"things could change.\"]\n",
        "\n",
        "# Tokenize(split) the sentences into words\n",
        "texts = [[text for text in doc.split()] for doc in documents]\n",
        "print(texts)\n",
        "\n",
        "# Create dictionary\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "\n",
        "# Get information about the dictionary\n",
        "print(dictionary)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['The', 'Saudis', 'are', 'preparing', 'a', 'report', 'that', 'will', 'acknowledge', 'that'], ['Saudi', 'journalist', 'Jamal', \"Khashoggi's\", 'death', 'was', 'the', 'result', 'of', 'an'], ['interrogation', 'that', 'went', 'wrong,', 'one', 'that', 'was', 'intended', 'to', 'lead'], ['to', 'his', 'abduction', 'from', 'Turkey,', 'according', 'to', 'two', 'sources.']]\n",
            "Dictionary<33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXo52UvdzGIv"
      },
      "source": [
        "As it says the dictionary has 33 unique tokens (or words). Let’s see the unique ids for each of these tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNWf4tnfzGIw",
        "outputId": "8b148bfc-16be-436f-cfc6-a2f6b0d902d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Show the word to id map\n",
        "print(dictionary.token2id)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Saudis': 0, 'The': 1, 'a': 2, 'acknowledge': 3, 'are': 4, 'preparing': 5, 'report': 6, 'that': 7, 'will': 8, 'Jamal': 9, \"Khashoggi's\": 10, 'Saudi': 11, 'an': 12, 'death': 13, 'journalist': 14, 'of': 15, 'result': 16, 'the': 17, 'was': 18, 'intended': 19, 'interrogation': 20, 'lead': 21, 'one': 22, 'to': 23, 'went': 24, 'wrong,': 25, 'Turkey,': 26, 'abduction': 27, 'according': 28, 'from': 29, 'his': 30, 'sources.': 31, 'two': 32}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO8QC5DWzGI2"
      },
      "source": [
        "We have successfully created a Dictionary object. Gensim will use this dictionary to create a bag-of-words corpus where the words in the documents are replaced with its respective id provided by this dictionary.\n",
        "\n",
        "If you get new documents in the future, it is also possible to update an existing dictionary to include the new words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8tVegRYzGI4",
        "outputId": "ab461272-60f3-417d-815d-ec971903b67e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "documents_2 = [\"The intersection graph of paths in trees\",\n",
        "               \"Graph minors IV Widths of trees and well quasi ordering\",\n",
        "               \"Graph minors A survey\"]\n",
        "\n",
        "texts_2 = [[text for text in doc.split()] for doc in documents_2]\n",
        "print(texts_2)\n",
        "\n",
        "dictionary.add_documents(texts_2)\n",
        "\n",
        "\n",
        "# If you check now, the dictionary should have been updated with the new words (tokens).\n",
        "print(dictionary)\n",
        "\n",
        "print(dictionary.token2id)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['The', 'intersection', 'graph', 'of', 'paths', 'in', 'trees'], ['Graph', 'minors', 'IV', 'Widths', 'of', 'trees', 'and', 'well', 'quasi', 'ordering'], ['Graph', 'minors', 'A', 'survey']]\n",
            "Dictionary<48 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...>\n",
            "{'Saudis': 0, 'The': 1, 'a': 2, 'acknowledge': 3, 'are': 4, 'preparing': 5, 'report': 6, 'that': 7, 'will': 8, 'Jamal': 9, \"Khashoggi's\": 10, 'Saudi': 11, 'an': 12, 'death': 13, 'journalist': 14, 'of': 15, 'result': 16, 'the': 17, 'was': 18, 'intended': 19, 'interrogation': 20, 'lead': 21, 'one': 22, 'to': 23, 'went': 24, 'wrong,': 25, 'Turkey,': 26, 'abduction': 27, 'according': 28, 'from': 29, 'his': 30, 'sources.': 31, 'two': 32, 'graph': 33, 'in': 34, 'intersection': 35, 'paths': 36, 'trees': 37, 'Graph': 38, 'IV': 39, 'Widths': 40, 'and': 41, 'minors': 42, 'ordering': 43, 'quasi': 44, 'well': 45, 'A': 46, 'survey': 47}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yKvje1vzGI9"
      },
      "source": [
        "Create a Dictionary from one file\n",
        "============================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjhYoo7OzGI-"
      },
      "source": [
        "You can also create a dictionary from a text file.\n",
        "\n",
        "The below example reads a file line-by-line and uses gensim’s simple_preprocess to process one line of the file at a time.\n",
        "\n",
        "The advantage here is it let’s you read an entire text file without loading the file in memory all at once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OMXgfCdzGI_"
      },
      "source": [
        "from gensim.utils import simple_preprocess\n",
        "import os\n",
        "\n",
        "# Create gensim dictionary form a single tet file, deacc=True -> remove accent marks from tokens\n",
        "dictionary = corpora.Dictionary(simple_preprocess(line, deacc=True) for line in open('Alice_lines_utf8.txt', encoding='utf-8'))\n",
        "\n",
        "# Token to Id map\n",
        "dictionary.token2id\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5irB256GzGJ0"
      },
      "source": [
        "Create the TFIDF matrix\n",
        "======================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bsbs2nmkzGJ1"
      },
      "source": [
        "The Term Frequency – Inverse Document Frequency(TF-IDF) is also a bag-of-words model but unlike the regular corpus, TFIDF down weights tokens (words) that appears frequently across documents.\n",
        "\n",
        "How is TFIDF computed?\n",
        "\n",
        "Tf-Idf is computed by multiplying a local component like term frequency (TF) with a global component, that is, inverse document frequency (IDF) and optionally normalizing the result to unit length.\n",
        "\n",
        "As a result of this, the words that occur frequently across documents will get downweighted.\n",
        "\n",
        "There are multiple variations of formulas for TF and IDF existing. Gensim uses the SMART Information retrieval system that can be used to implement these variations. You can specify what formula to use specifying the smartirs parameter in the TfidfModel. See help(models.TfidfModel) for more details.\n",
        "\n",
        "So, how to get the TFIDF weights?\n",
        "\n",
        "By training the corpus with models.TfidfModel(). Then, apply the corpus within the square brackets of the trained tfidf model. See example below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuCYYEmizGJ2",
        "outputId": "3ff56c4a-dfa8-41a1-e362-96e8c4d98448",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from gensim import models\n",
        "import numpy as np\n",
        "\n",
        "documents = [\"This is the first line\",\n",
        "             \"This is the second sentence\",\n",
        "             \"This third document\"]\n",
        "\n",
        "# Create the Dictionary and Corpus\n",
        "mydict = corpora.Dictionary([simple_preprocess(line) for line in documents])\n",
        "print(mydict)\n",
        "corpus = [mydict.doc2bow(simple_preprocess(line), allow_update=True) for line in documents]\n",
        "print(corpus)\n",
        "\n",
        "# Show the Word Weights in Corpus\n",
        "for doc in corpus:\n",
        "    print([[mydict[id], freq] for id, freq in doc])\n",
        "\n",
        "print('======TF-IDF======')\n",
        "# Create the TF-IDF model\n",
        "tfidf = models.TfidfModel(corpus)\n",
        "\n",
        "# Show the TF-IDF weights\n",
        "for doc in tfidf[corpus]:\n",
        "    print([[mydict[id], np.around(freq, decimals=2)] for id, freq in doc])\n",
        "\n",
        "print(tfidf[corpus[0]] )"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary<9 unique tokens: ['first', 'is', 'line', 'the', 'this']...>\n",
            "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)], [(1, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(4, 1), (7, 1), (8, 1)]]\n",
            "[['first', 1], ['is', 1], ['line', 1], ['the', 1], ['this', 1]]\n",
            "[['is', 1], ['the', 1], ['this', 1], ['second', 1], ['sentence', 1]]\n",
            "[['this', 1], ['document', 1], ['third', 1]]\n",
            "======TF-IDF======\n",
            "[['first', 0.66], ['is', 0.24], ['line', 0.66], ['the', 0.24]]\n",
            "[['is', 0.24], ['the', 0.24], ['second', 0.66], ['sentence', 0.66]]\n",
            "[['document', 0.71], ['third', 0.71]]\n",
            "[(0, 0.6633689723434505), (1, 0.2448297500958463), (2, 0.6633689723434505), (3, 0.2448297500958463)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbPyRMsEzGJ8"
      },
      "source": [
        "Notice the difference in weights of the words between the original corpus and the tfidf weighted corpus.\n",
        "\n",
        "The words ‘is’ and ‘the’ occur in two documents and were weighted down. The word ‘this’ appearing in all three documents was removed altogether. In simple terms, words that occur more frequently across the documents get smaller weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpNOwBxDzGKB"
      },
      "source": [
        "Gensim provides an inbuilt API to download popular text datasets and word embedding models.\n",
        "\n",
        "A comprehensive list of available datasets and models is maintained here: https://raw.githubusercontent.com/RaRe-Technologies/gensim-data/master/list.json.\n",
        "\n",
        "Using the API to download the dataset is as simple as calling the api.load() method with the right data or model name.\n",
        "\n",
        "Now you know how to download datasets and pre-trained models with gensim.\n",
        "\n",
        "Let’s download the text8 dataset, which is nothing but the “First 100,000,000 bytes of plain text from Wikipedia”. Then, from this, we will generate our word2vec model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjfJgwO8zGKQ"
      },
      "source": [
        "Train Word2Vec model using gensim\n",
        "================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8In_GE9zGKQ"
      },
      "source": [
        "A word embedding model is a model that can provide numerical vectors for a given word. Using the Gensim’s downloader API, you can download pre-built word embedding models like word2vec, fasttext, GloVe and ConceptNet. These are built on large corpuses of commonly occurring text data such as wikipedia, google news etc.\n",
        "\n",
        "However, if you are working in a specialized niche such as technical documents, you may not able to get word embeddings for all the words. So, in such cases its desirable to train your own model.\n",
        "\n",
        "Gensim’s Word2Vec implementation let’s you train your own word embedding model for a given corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNcWU-MazGKR",
        "outputId": "e559dbe1-e651-4258-9a02-1f18c260d867",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from gensim.models.word2vec import Word2Vec\n",
        "from multiprocessing import cpu_count\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Download dataset\n",
        "dataset = api.load(\"text8\")\n",
        "data = [d for d in dataset]\n",
        "\n",
        "# Split the data into 2 parts.\n",
        "data_part1 = data[:1000]\n",
        "\n",
        "# Train Word2Vec model. Defaults result vector size = 100\n",
        "model = Word2Vec(data_part1, window=5, min_count = 2, sg=0, workers=cpu_count())\n",
        "\n",
        "# Get the word vector for given word\n",
        "model.wv['topic']"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.2703217 , -0.8083792 , -0.2925763 ,  0.433493  , -0.8865123 ,\n",
              "       -0.6884289 , -1.3115997 ,  1.1001226 ,  0.808066  , -0.2905409 ,\n",
              "        0.839751  , -0.626363  , -0.49549112, -1.5518533 , -0.7057076 ,\n",
              "        0.90056425, -0.12318409, -0.53909725,  0.2051187 , -0.26116267,\n",
              "        0.00583857,  1.57774   , -0.580765  ,  0.5894798 , -0.82096   ,\n",
              "       -0.1604243 ,  0.06158253,  0.3948521 , -0.18716957,  0.7764375 ,\n",
              "        0.23271777,  0.31557995, -0.24596128, -1.2741156 ,  0.03714888,\n",
              "       -0.6501605 , -0.81270087, -1.2894961 ,  0.99802935, -0.21924587,\n",
              "        0.40310413, -0.04981372, -0.27114603, -0.11670995,  0.31651703,\n",
              "       -0.15808582, -0.22778812, -0.08785888, -0.41514707,  0.4629715 ,\n",
              "       -0.6558279 ,  0.4093873 ,  0.82389724, -0.16391252, -1.0488547 ,\n",
              "        0.14271432, -1.4065292 ,  0.1945657 , -1.8327117 ,  0.8029601 ,\n",
              "        0.11568496,  0.77454966,  0.3071646 ,  0.09257562, -0.35892737,\n",
              "        0.10722373,  0.19503492, -0.30807117, -0.7341304 ,  0.62717354,\n",
              "       -0.7296574 , -0.45431146,  0.31238028, -0.5069458 ,  0.0720349 ,\n",
              "       -0.44895718, -0.47498557,  0.24307726, -0.516046  ,  0.2017596 ,\n",
              "        0.875888  ,  0.19282998, -0.5009788 ,  0.3694916 ,  0.71563065,\n",
              "        0.00662866, -1.9983662 ,  1.3104837 ,  0.5044752 ,  0.25533164,\n",
              "       -0.7950373 ,  0.39428067,  0.7161219 , -0.53559744,  0.9931729 ,\n",
              "        0.7560237 , -0.02949442,  0.24250275, -0.22662248,  0.65082455],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cD-nQkezGKV",
        "outputId": "531829bf-3087-44e4-ecdc-651456b1e0af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#get similar words\n",
        "model.wv.most_similar('topic')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('discussion', 0.7371512651443481),\n",
              " ('focus', 0.7094348073005676),\n",
              " ('interpretation', 0.7049460411071777),\n",
              " ('discourse', 0.7043935656547546),\n",
              " ('debate', 0.6989267468452454),\n",
              " ('speculation', 0.6963982582092285),\n",
              " ('premise', 0.6891854405403137),\n",
              " ('consensus', 0.6872879862785339),\n",
              " ('explanation', 0.680461585521698),\n",
              " ('focuses', 0.6751286387443542)]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayC5x1l_zGKZ"
      },
      "source": [
        "# Save and Load Model\n",
        "model.save('newmodel')\n",
        "model = Word2Vec.load('newmodel')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl4WuZChzGKw",
        "outputId": "1dc80c04-99a5-45c5-9637-ae869edf1d78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get the word vector for given word\n",
        "model.wv['cat']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.10726921,  0.17539935, -1.6782835 ,  0.88824236, -1.0143253 ,\n",
              "       -0.58637846,  1.2031175 ,  0.9354335 , -0.08056835, -0.99758744,\n",
              "        0.25955707,  0.6650942 , -0.07427248,  0.55190355,  0.90653425,\n",
              "       -0.16370137, -1.0263704 , -0.60833365,  0.35087383,  1.2698172 ,\n",
              "       -1.591476  , -0.45269197, -0.5508202 ,  0.35510886, -1.3410037 ,\n",
              "       -0.13008042, -0.4552002 , -0.17718093, -0.16941968, -1.0512463 ,\n",
              "        1.5862764 , -0.36579835,  0.04987056, -0.44437885, -1.4062667 ,\n",
              "       -0.5154404 , -0.9602719 ,  0.29555872, -0.40189412, -0.97448653,\n",
              "        1.1201429 ,  0.08775884, -0.36510828, -2.4648805 ,  0.25924042,\n",
              "       -1.0812641 , -0.45518818,  0.02472565,  0.2062556 , -1.5449716 ,\n",
              "        1.6602125 ,  0.8662719 , -2.1257217 ,  0.05641563, -0.09212045,\n",
              "       -0.9565714 , -0.9600496 , -0.609136  , -1.0849751 ,  0.1684088 ,\n",
              "        0.4651139 , -1.4281328 ,  0.48215285,  0.53489006, -0.2632438 ,\n",
              "        1.4148672 ,  0.12195655,  0.82940644,  0.19183348, -0.15850142,\n",
              "        0.03459918, -0.06072848,  0.8672215 , -0.60581   ,  0.23169234,\n",
              "        0.24828927, -0.6276974 ,  0.4193052 ,  0.3712892 , -0.19592592,\n",
              "       -0.95387197,  0.20381786, -2.929506  ,  0.83115345,  0.23100612,\n",
              "       -1.2481703 , -0.614117  ,  0.44052327,  1.2550447 , -1.1358486 ,\n",
              "        1.1064558 ,  0.06115614, -0.36035797, -0.5261687 ,  0.93989533,\n",
              "        0.36663422,  0.02777792, -0.4372335 ,  0.07148087, -0.32872787],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVoaEF_dzGK0",
        "outputId": "97ccb7a4-b2e1-4e85-d725-43ffc6665690",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#get similar words\n",
        "model.wv.most_similar('cat')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('dog', 0.8032337427139282),\n",
              " ('bee', 0.7688228487968445),\n",
              " ('sweet', 0.7592843174934387),\n",
              " ('flower', 0.7481402158737183),\n",
              " ('goat', 0.7353066205978394),\n",
              " ('blonde', 0.7340176701545715),\n",
              " ('dogs', 0.7266868948936462),\n",
              " ('bird', 0.7220409512519836),\n",
              " ('honey', 0.7205797433853149),\n",
              " ('bear', 0.7186970710754395)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAJb-6sizGK3",
        "outputId": "bdd8c05f-56a0-4092-91af-b922ba4cf362",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#get similarity between two words\n",
        "model.wv.similarity('dog','cat')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8032337"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh-8EvnnzGK7"
      },
      "source": [
        "Import pre-trainined word2vec\n",
        "============================"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOmrWPsVzGK9"
      },
      "source": [
        "We just saw how to get the word vectors for Word2Vec model we just trained. However, gensim lets you download state of the art pretrained models through the downloader API. Let’s see how to extract the word vectors from a couple of these models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSYjX31vzGK-"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Download the models (1660MB)\n",
        "#word2vec_model300 = api.load('word2vec-google-news-100')\n",
        "\n",
        "#get similar words\n",
        "#word2vec_model300.wv.most_similar('support')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viKWnzRfzGLB",
        "outputId": "9983f13d-876c-4bc5-b522-cde9e5ed4884",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#download a model based on Glove (128MB)\n",
        "import gensim.downloader as api\n",
        "glove_model100 = api.load('glove-wiki-gigaword-100')\n",
        "#get similar words\n",
        "glove_model100.most_similar('dog')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cat', 0.8798074126243591),\n",
              " ('dogs', 0.8344309329986572),\n",
              " ('pet', 0.7449564337730408),\n",
              " ('puppy', 0.723637580871582),\n",
              " ('horse', 0.7109653949737549),\n",
              " ('animal', 0.6817063093185425),\n",
              " ('pig', 0.655417263507843),\n",
              " ('boy', 0.6545308232307434),\n",
              " ('cats', 0.6471932530403137),\n",
              " ('rabbit', 0.6468630433082581)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_M1OKhBaL4X",
        "outputId": "1085584c-6c98-41bd-9f8d-7414759a61ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "glove_model100.similarity('dog','cat')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8798075"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example of use"
      ],
      "metadata": {
        "id": "3T1VFhv1tOMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download Dataset\n",
        "import pandas as pd\n",
        "url = 'https://bit.ly/2CdYYuf'\n",
        "yelp = pd.read_csv(url, sep='\\t', header = None)\n",
        "yelp.rename(columns={0:'Reviews', 1:'Sentiment'}, inplace=True)\n",
        "yelp.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ncNYNyKJtP5K",
        "outputId": "fcf7baee-596b-4277-fed6-58cd0a6e13ab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             Reviews  Sentiment\n",
              "0                           Wow... Loved this place.          1\n",
              "1                                 Crust is not good.          0\n",
              "2          Not tasty and the texture was just nasty.          0\n",
              "3  Stopped by during the late May bank holiday of...          1\n",
              "4  The selection on the menu was great and so wer...          1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1c345dbd-093c-47b2-8d41-e2d9d281646f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Reviews</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1c345dbd-093c-47b2-8d41-e2d9d281646f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1c345dbd-093c-47b2-8d41-e2d9d281646f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1c345dbd-093c-47b2-8d41-e2d9d281646f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ccf2dbf9-e06c-42c2-a550-ee2f932038aa\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ccf2dbf9-e06c-42c2-a550-ee2f932038aa')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ccf2dbf9-e06c-42c2-a550-ee2f932038aa button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download a model based on Glove (128MB)\n",
        "import gensim.downloader as api\n",
        "glove_model100 = api.load('glove-wiki-gigaword-100')\n"
      ],
      "metadata": {
        "id": "hFLZffukuBYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_model100.get_vector(\"office\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjKK4ulcuiKe",
        "outputId": "1984a6d8-141e-44db-f0a3-ebfee04a1733"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.083012, -0.74463 ,  0.36394 , -0.04265 ,  0.60577 ,  0.13998 ,\n",
              "       -0.50061 ,  0.90389 ,  0.41351 ,  0.49011 ,  0.10642 , -0.62883 ,\n",
              "        0.31716 ,  0.77279 , -0.22061 , -0.13117 ,  0.59952 ,  0.40445 ,\n",
              "       -0.52231 , -0.42995 ,  0.075281,  0.28239 ,  0.014645, -0.32397 ,\n",
              "       -0.74076 , -0.80056 ,  0.23731 , -0.49243 , -0.32606 , -0.20385 ,\n",
              "        0.93649 ,  0.22245 ,  0.25503 ,  0.61261 , -0.49376 ,  0.84066 ,\n",
              "       -0.57353 ,  0.053669,  0.29911 , -0.21548 , -0.22307 , -0.58031 ,\n",
              "        0.36928 , -0.34358 ,  0.30455 , -0.14287 , -0.38094 , -0.53703 ,\n",
              "        0.1597  , -0.43649 ,  0.42691 , -1.0276  ,  0.38602 ,  1.0371  ,\n",
              "       -0.18697 , -2.4962  , -0.37856 ,  0.16619 ,  1.953   ,  0.47491 ,\n",
              "       -0.49005 , -0.2078  , -0.033339,  0.23562 ,  0.18506 , -0.41896 ,\n",
              "        0.50037 ,  0.41745 ,  0.51059 ,  0.59109 ,  0.02061 , -0.093909,\n",
              "       -0.47164 , -0.89987 ,  0.22922 , -0.13374 , -0.28564 ,  0.44327 ,\n",
              "       -1.5182  , -0.076197,  0.37112 ,  0.14877 , -0.030087,  0.10301 ,\n",
              "       -0.92582 , -0.22484 ,  0.43015 ,  0.078164,  0.074104, -0.81831 ,\n",
              "        0.81235 , -0.23451 ,  0.43832 ,  0.49256 , -0.1696  ,  0.34038 ,\n",
              "        0.36897 ,  0.10641 ,  0.43051 , -0.036887], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "nltk.download('stopwords')\n",
        "docs_vectors = pd.DataFrame() # creating empty final dataframe\n",
        "stopwords = nltk.corpus.stopwords.words('english') # removing stop words\n",
        "\n",
        "for doc in yelp['Reviews'].str.lower().str.replace('[^a-z ]', ''): # looping through each document and cleaning it\n",
        "    temp = pd.DataFrame()  # creating a temporary dataframe(store value for 1st doc & for 2nd doc remove the details of 1st & proced through 2nd and so on..)\n",
        "    for word in doc.split(' '): # looping through each word of a single document and spliting through space - You can use a Tokenizer\n",
        "        if word not in stopwords: # if word is not present in stopwords then (try)\n",
        "            try:\n",
        "                word_vec = glove_model100.get_vector(word) # if word is present in embeddings(goole provides weights associate with words(300)) then proceed\n",
        "                temp = temp.append(pd.Series(word_vec), ignore_index = True) # if word is present then append it to temporary dataframe\n",
        "            except:\n",
        "                print(\"The token: \"+word+\" not in Embedding Space Vocabulary\")\n",
        "    doc_vector = temp.mean() # take the average of each column(w0, w1, w2,........w300)\n",
        "    docs_vectors = docs_vectors.append(doc_vector, ignore_index = True) # append each document value to the final dataframe\n",
        "docs_vectors.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vZj75w1tmQe",
        "outputId": "c0805715-5403-4a99-95df-0b61c6c25841"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token: honeslty not in Embedding Space Vocabulary\n",
            "The token: wayyy not in Embedding Space Vocabulary\n",
            "The token: ravoli not in Embedding Space Vocabulary\n",
            "The token: chickenwith not in Embedding Space Vocabulary\n",
            "The token: cranberrymmmm not in Embedding Space Vocabulary\n",
            "The token: burrittos not in Embedding Space Vocabulary\n",
            "The token: rightthe not in Embedding Space Vocabulary\n",
            "The token: cakeohhh not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: itfriendly not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: updatewent not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: bloddy not in Embedding Space Vocabulary\n",
            "The token: tigerlilly not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: delish not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: handsdown not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: topvery not in Embedding Space Vocabulary\n",
            "The token: serivce not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: delish not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: beateous not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: pepperand not in Embedding Space Vocabulary\n",
            "The token: muststop not in Embedding Space Vocabulary\n",
            "The token: worstannoying not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: theyd not in Embedding Space Vocabulary\n",
            "The token: mayowell not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: flavourful not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: ayce not in Embedding Space Vocabulary\n",
            "The token: workingeating not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: wouldve not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: veggitarian not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: saganaki not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: yelpers not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: delish not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: servicecheck not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: shawarrrrrrma not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: vegasthere not in Embedding Space Vocabulary\n",
            "The token: classywarm not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: shouldnt not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: andddd not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: transcendant not in Embedding Space Vocabulary\n",
            "The token: buldogis not in Embedding Space Vocabulary\n",
            "The token: meeverything not in Embedding Space Vocabulary\n",
            "The token: beensteppedinandtrackedeverywhere not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: expertconnisseur not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: absolutley not in Embedding Space Vocabulary\n",
            "The token: breakfastlunch not in Embedding Space Vocabulary\n",
            "The token: gooodd not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: nargile not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: profiterole not in Embedding Space Vocabulary\n",
            "The token: carlys not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: foodand not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: cavier not in Embedding Space Vocabulary\n",
            "The token: mebunch not in Embedding Space Vocabulary\n",
            "The token: noca not in Embedding Space Vocabulary\n",
            "The token: cheesecurds not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: upway not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: vinegrette not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: perpared not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: veganveggie not in Embedding Space Vocabulary\n",
            "The token: crumby not in Embedding Space Vocabulary\n",
            "The token: itll not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: soooooo not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: noncustomer not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: highquality not in Embedding Space Vocabulary\n",
            "The token: macarons not in Embedding Space Vocabulary\n",
            "The token: werent not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: booksomethats not in Embedding Space Vocabulary\n",
            "The token: falafels not in Embedding Space Vocabulary\n",
            "The token: ganoush not in Embedding Space Vocabulary\n",
            "The token: smashburger not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: amazingrge not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: auju not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: specialand not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: vinegrette not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: nonfancy not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: restaraunt not in Embedding Space Vocabulary\n",
            "The token: satifying not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: hereas not in Embedding Space Vocabulary\n",
            "The token: pissd not in Embedding Space Vocabulary\n",
            "The token: goldencrispy not in Embedding Space Vocabulary\n",
            "The token: crpe not in Embedding Space Vocabulary\n",
            "The token: overhip not in Embedding Space Vocabulary\n",
            "The token: underservices not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: gloveseverything not in Embedding Space Vocabulary\n",
            "The token: costcos not in Embedding Space Vocabulary\n",
            "The token: ownerchef not in Embedding Space Vocabulary\n",
            "The token: albondigas not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: pured not in Embedding Space Vocabulary\n",
            "The token: postinos not in Embedding Space Vocabulary\n",
            "The token: itdefinitely not in Embedding Space Vocabulary\n",
            "The token: waaaaaayyyyyyyyyy not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: chipolte not in Embedding Space Vocabulary\n",
            "The token: douchey not in Embedding Space Vocabulary\n",
            "The token: deuchebaggery not in Embedding Space Vocabulary\n",
            "The token: herewhat not in Embedding Space Vocabulary\n",
            "The token: yaall not in Embedding Space Vocabulary\n",
            "The token: disapppointment not in Embedding Space Vocabulary\n",
            "The token: prettyoff not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token:  not in Embedding Space Vocabulary\n",
            "The token: hadnt not in Embedding Space Vocabulary\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs_vectors[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLDmwSV-vo6w",
        "outputId": "aaab8275-0730-4bc9-d4fd-4c3c666e6aa7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      0.367257\n",
            "1      0.297900\n",
            "2      0.394870\n",
            "3     -0.346915\n",
            "4      0.281877\n",
            "         ...   \n",
            "995    0.703294\n",
            "996    0.331586\n",
            "997    0.133566\n",
            "998    0.371684\n",
            "999    0.360034\n",
            "Name: 1, Length: 1000, dtype: float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(docs_vectors,\n",
        "                                                   yelp['Sentiment'],\n",
        "                                                   test_size = 0.2,\n",
        "                                                   random_state = 1)\n",
        "train_x.shape, train_y.shape, test_x.shape, test_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxdYmsv_vxmu",
        "outputId": "e090e0d3-70de-47a7-c889-c1ad97baa14d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((800, 100), (800,), (200, 100), (200,))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GaussianNB()\n",
        "model.fit(train_x, train_y)\n",
        "test_pred = model.predict(test_x)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(test_y, test_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YCp9ORNwNlc",
        "outputId": "91c16adc-6f5c-4caf-bbba-25f73946b646"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.76      0.72       108\n",
            "           1       0.68      0.60      0.64        92\n",
            "\n",
            "    accuracy                           0.69       200\n",
            "   macro avg       0.68      0.68      0.68       200\n",
            "weighted avg       0.68      0.69      0.68       200\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
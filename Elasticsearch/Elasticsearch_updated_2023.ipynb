{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Basic Elastic Test!</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55.58s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.0.6-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.0-cp310-cp310-macosx_10_9_x86_64.whl (119 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2023.7.22 charset-normalizer-3.3.0 idna-3.4 requests-2.31.0 urllib3-2.0.6\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json \n",
    "#MY ELASTICSEARCH URL\n",
    "URL = \"http://193.204.187.52:9201/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\" : \"host52.di.uniba.it\",\n",
      "  \"cluster_name\" : \"elasticsearch\",\n",
      "  \"cluster_uuid\" : \"TxUkq84bRKm7Raxu9J4Gsg\",\n",
      "  \"version\" : {\n",
      "    \"number\" : \"8.10.3\",\n",
      "    \"build_flavor\" : \"default\",\n",
      "    \"build_type\" : \"tar\",\n",
      "    \"build_hash\" : \"c63272efed16b5a1c25f3ce500715b7fddf9a9fb\",\n",
      "    \"build_date\" : \"2023-10-05T10:15:55.152563867Z\",\n",
      "    \"build_snapshot\" : false,\n",
      "    \"lucene_version\" : \"9.7.0\",\n",
      "    \"minimum_wire_compatibility_version\" : \"7.17.0\",\n",
      "    \"minimum_index_compatibility_version\" : \"7.0.0\"\n",
      "  },\n",
      "  \"tagline\" : \"You Know, for Search\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sending post request and saving response as response object \n",
    "r = requests.get(url = URL) \n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"_index\":\"test-data\",\"_id\":\"21\",\"_version\":1,\"result\":\"created\",\"_shards\":{\"total\":2,\"successful\":1,\"failed\":0},\"_seq_no\":0,\"_primary_term\":1}\n"
     ]
    }
   ],
   "source": [
    "r = requests.put(url=URL+'test-data/_doc/21',json ={\"rank\": 21,\"city\": \"Boston\",\"state\": \"Massachusetts\", \"population2010\": 617594,\"land_area\": 48.277,\"location\": {\"lat\": 42.332,\"lon\": 71.0202 }, \"abbreviation\": \"MA\"})\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response is:{\"_index\":\"test-data\",\"_id\":\"21\",\"_version\":1,\"_seq_no\":0,\"_primary_term\":1,\"found\":true,\"_source\":{\"rank\": 21, \"city\": \"Boston\", \"state\": \"Massachusetts\", \"population2010\": 617594, \"land_area\": 48.277, \"location\": {\"lat\": 42.332, \"lon\": 71.0202}, \"abbreviation\": \"MA\"}}\n"
     ]
    }
   ],
   "source": [
    "# sending post request and saving response as response object \n",
    "r = requests.get(url=URL+'test-data/_doc/21') \n",
    "  \n",
    "# extracting response text  \n",
    "response = r.text \n",
    "print(\"The response is:%s\"%response) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"_index\": \"test-data\",\n",
      "    \"_id\": \"21\",\n",
      "    \"_version\": 1,\n",
      "    \"_seq_no\": 0,\n",
      "    \"_primary_term\": 1,\n",
      "    \"found\": true,\n",
      "    \"_source\": {\n",
      "        \"rank\": 21,\n",
      "        \"city\": \"Boston\",\n",
      "        \"state\": \"Massachusetts\",\n",
      "        \"population2010\": 617594,\n",
      "        \"land_area\": 48.277,\n",
      "        \"location\": {\n",
      "            \"lat\": 42.332,\n",
      "            \"lon\": 71.0202\n",
      "        },\n",
      "        \"abbreviation\": \"MA\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "y = json.loads(response)\n",
    "# pretty printing data\n",
    "pretty_data = json.dumps(y, indent=4)\n",
    " \n",
    "print(pretty_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"_index\":\"test-data\",\"_id\":\"35\",\"_version\":1,\"result\":\"created\",\"_shards\":{\"total\":2,\"successful\":1,\"failed\":0},\"_seq_no\":1,\"_primary_term\":1}\n"
     ]
    }
   ],
   "source": [
    "r = requests.put(url=URL+'test-data/_doc/35',json ={\"line_id\":2,\"play_name\":\"Henry IV\", \"speech_number\":\"\",\"line_number\":\"\",\"speaker\":\"\",\"text_entry\":\"SCENE I. London. The palace.\" })\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Henry IV\n"
     ]
    }
   ],
   "source": [
    "# sending post request and saving response as response object \n",
    "r = requests.get(url=URL+'test-data/_doc/35') \n",
    "y =json.loads(r.text)\n",
    "# pretty printing data\n",
    "pretty_data = json.dumps(y, indent=4)\n",
    "# the result is a Python dictionary:\n",
    "print(y[\"_source\"][\"play_name\"]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"_index\": \"test-data\",\n",
      "    \"_id\": \"35\",\n",
      "    \"_version\": 1,\n",
      "    \"_seq_no\": 1,\n",
      "    \"_primary_term\": 1,\n",
      "    \"found\": true,\n",
      "    \"_source\": {\n",
      "        \"line_id\": 2,\n",
      "        \"play_name\": \"Henry IV\",\n",
      "        \"speech_number\": \"\",\n",
      "        \"line_number\": \"\",\n",
      "        \"speaker\": \"\",\n",
      "        \"text_entry\": \"SCENE I. London. The palace.\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(pretty_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"_index\":\"test-data\",\"_id\":\"21\",\"_version\":2,\"result\":\"deleted\",\"_shards\":{\"total\":2,\"successful\":1,\"failed\":0},\"_seq_no\":2,\"_primary_term\":1}\n"
     ]
    }
   ],
   "source": [
    "r = requests.delete(url=URL+'test-data/_doc/21')\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"shakespeare\"}\n"
     ]
    }
   ],
   "source": [
    "r = requests.put(url=URL+'shakespeare', json = { \n",
    "\"mappings\" : {\n",
    "    \"properties\" : { \n",
    "        \"speaker\" : {\"type\": \"keyword\" },\n",
    "        \"play_name\" : {\"type\": \"keyword\" },\n",
    "        \"line_id\" : { \"type\" : \"integer\" },\n",
    "        \"speech_number\" : { \"type\" : \"integer\" },\n",
    "        \"text_entry\" : {\"type\": \"text\" }\n",
    "    }\n",
    "   }\n",
    "}\n",
    ")\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"logstash-2015.05.18\"}\n"
     ]
    }
   ],
   "source": [
    "r = requests.put(url=URL+'logstash-2015.05.18', json = {\n",
    "   \"mappings\": {\n",
    "         \"properties\": { \n",
    "              \"geo\": { \n",
    "                 \"properties\": { \n",
    "                     \"coordinates\": { \"type\": \"geo_point\" } \n",
    "                 }\n",
    "              },\n",
    "            \t\"ip\" : {\"type\": \"ip\" },\n",
    "            \t\"@timestamp\" : {\"type\": \"date\" }\n",
    "          }\n",
    "     } \n",
    "   }\n",
    ")\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"logstash-2015.05.19\"}\n"
     ]
    }
   ],
   "source": [
    "r = requests.put(url=URL+'logstash-2015.05.19')\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"logstash-2015.05.20\"}\n"
     ]
    }
   ],
   "source": [
    "r = requests.put(url=URL+'logstash-2015.05.20')\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"acknowledged\":true}\n"
     ]
    }
   ],
   "source": [
    "r = requests.delete(url=URL+'bank')\n",
    "print(r.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Let's try a search request</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Request URL\n",
    "R_URL = URL+\"shakespeare/_search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PAYLOAD\n",
    "R_data =\"{\\\"query\\\": { \\\"match\\\" : { \\\"text_entry\\\" : {\\\"query\\\" : \\\"knife war edge\\\",\\\"operator\\\" : \\\"and\\\"}}} }\"\n",
    "JSON_DATA = y = json.loads(R_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(url = R_URL, json = JSON_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response is:{\"took\":756,\"timed_out\":false,\"_shards\":{\"total\":1,\"successful\":1,\"skipped\":0,\"failed\":0},\"hits\":{\"total\":{\"value\":1,\"relation\":\"eq\"},\"max_score\":20.082003,\"hits\":[{\"_index\":\"shakespeare\",\"_id\":\"19\",\"_score\":20.082003,\"_source\":{\"line_id\":20,\"play_name\":\"Henry IV\",\"speech_number\":1,\"line_number\":\"1.1.17\",\"speaker\":\"KING HENRY IV\",\"text_entry\":\"The edge of war, like an ill-sheathed knife,\"}}]}}\n"
     ]
    }
   ],
   "source": [
    "response  = response.text \n",
    "print(\"The response is:%s\"%response) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'took': 756, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 1, 'relation': 'eq'}, 'max_score': 20.082003, 'hits': [{'_index': 'shakespeare', '_id': '19', '_score': 20.082003, '_source': {'line_id': 20, 'play_name': 'Henry IV', 'speech_number': 1, 'line_number': '1.1.17', 'speaker': 'KING HENRY IV', 'text_entry': 'The edge of war, like an ill-sheathed knife,'}}]}}\n",
      "{\n",
      "    \"took\": 756,\n",
      "    \"timed_out\": false,\n",
      "    \"_shards\": {\n",
      "        \"total\": 1,\n",
      "        \"successful\": 1,\n",
      "        \"skipped\": 0,\n",
      "        \"failed\": 0\n",
      "    },\n",
      "    \"hits\": {\n",
      "        \"total\": {\n",
      "            \"value\": 1,\n",
      "            \"relation\": \"eq\"\n",
      "        },\n",
      "        \"max_score\": 20.082003,\n",
      "        \"hits\": [\n",
      "            {\n",
      "                \"_index\": \"shakespeare\",\n",
      "                \"_id\": \"19\",\n",
      "                \"_score\": 20.082003,\n",
      "                \"_source\": {\n",
      "                    \"line_id\": 20,\n",
      "                    \"play_name\": \"Henry IV\",\n",
      "                    \"speech_number\": 1,\n",
      "                    \"line_number\": \"1.1.17\",\n",
      "                    \"speaker\": \"KING HENRY IV\",\n",
      "                    \"text_entry\": \"The edge of war, like an ill-sheathed knife,\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Convert to JSON\n",
    "y = json.loads(response)\n",
    "print(y)\n",
    "# pretty printing data\n",
    "pretty_data = json.dumps(y, indent=4)\n",
    " \n",
    "print(pretty_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The edge of war, like an ill-sheathed knife,\n"
     ]
    }
   ],
   "source": [
    "#Print the specific field\n",
    "print(y[\"hits\"][\"hits\"][0][\"_source\"][\"text_entry\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Lercio Crawling</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define requests object\n",
    "import requests\n",
    "import json \n",
    "#MY ELASTICSEARCH URL\n",
    "URL = \"http://127.0.0.1:9201/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"acknowledged\": true,\n",
      "    \"shards_acknowledged\": true,\n",
      "    \"index\": \"lercio\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#New Lercio Index\n",
    "index = \"{ \\\"mappings\\\":{\\\"properties\\\":{\\\"title\\\" : {\\\"type\\\": \\\"text\\\" }, \\\"url\\\" : {\\\"type\\\": \\\"keyword\\\" },  \\\"author\\\" : { \\\"type\\\" : \\\"text\\\" }, \\\"abstract\\\" : { \\\"type\\\" : \\\"text\\\" }}}}\"\n",
    "JSON_DATA = json.loads(index)\n",
    "\n",
    "#Request URL\n",
    "R_URL = URL+\"lercio\"\n",
    "RINS_URL = URL+\"lercio/_doc/\"\n",
    "\n",
    "response = requests.put(url = R_URL, json = JSON_DATA)\n",
    "response  = response.text \n",
    "\n",
    "#Convert to JSON\n",
    "y = json.loads(response)\n",
    "# pretty printing data\n",
    "pretty_data = json.dumps(y, indent=4)\n",
    "print(pretty_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in /usr/local/lib/python3.10/site-packages (2.6.2)\n",
      "Requirement already satisfied: service-identity>=16.0.0 in /usr/local/lib/python3.10/site-packages (from scrapy) (23.1.0)\n",
      "Requirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.10/site-packages (from scrapy) (4.9.3)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.10/site-packages (from scrapy) (2.1.2)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.10/site-packages (from scrapy) (1.6.2)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.10/site-packages (from scrapy) (2.0.7)\n",
      "Requirement already satisfied: tldextract in /usr/local/lib/python3.10/site-packages (from scrapy) (3.6.0)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in /usr/local/lib/python3.10/site-packages (from scrapy) (0.8.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from scrapy) (67.6.1)\n",
      "Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.10/site-packages (from scrapy) (1.8.1)\n",
      "Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.10/site-packages (from scrapy) (37.0.4)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.10/site-packages (from scrapy) (1.2.0)\n",
      "Requirement already satisfied: Twisted>=17.9.0 in /usr/local/lib/python3.10/site-packages (from scrapy) (22.10.0)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in /usr/local/lib/python3.10/site-packages (from scrapy) (6.1)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in /usr/local/lib/python3.10/site-packages (from scrapy) (22.0.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.10/site-packages (from scrapy) (0.3.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in /usr/local/lib/python3.10/site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/site-packages (from cryptography>=2.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in /usr/local/lib/python3.10/site-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
      "Requirement already satisfied: packaging in /Users/uniba/Library/Python/3.10/lib/python/site-packages (from parsel>=1.5.0->scrapy) (23.1)\n",
      "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.10/site-packages (from service-identity>=16.0.0->scrapy) (0.5.0)\n",
      "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/site-packages (from service-identity>=16.0.0->scrapy) (0.3.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/site-packages (from service-identity>=16.0.0->scrapy) (23.1.0)\n",
      "Requirement already satisfied: Automat>=0.8.0 in /usr/local/lib/python3.10/site-packages (from Twisted>=17.9.0->scrapy) (22.10.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.10/site-packages (from Twisted>=17.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: incremental>=21.3.0 in /usr/local/lib/python3.10/site-packages (from Twisted>=17.9.0->scrapy) (22.10.0)\n",
      "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.10/site-packages (from Twisted>=17.9.0->scrapy) (15.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.10/site-packages (from Twisted>=17.9.0->scrapy) (4.8.0)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/site-packages (from tldextract->scrapy) (3.4)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/site-packages (from tldextract->scrapy) (3.12.4)\n",
      "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/site-packages (from tldextract->scrapy) (1.5.1)\n",
      "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/site-packages (from tldextract->scrapy) (2.31.0)\n",
      "Requirement already satisfied: six in /Users/uniba/Library/Python/3.10/lib/python/site-packages (from Automat>=0.8.0->Twisted>=17.9.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.0->scrapy) (2.21)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.1.0->tldextract->scrapy) (3.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.1.0->tldextract->scrapy) (2023.7.22)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "%pip install scrapy\n",
    "import attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyopenssl==22.0.0 in /usr/local/lib/python3.10/site-packages (22.0.0)\n",
      "Requirement already satisfied: cryptography>=35.0 in /usr/local/lib/python3.10/site-packages (from pyopenssl==22.0.0) (37.0.4)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/site-packages (from cryptography>=35.0->pyopenssl==22.0.0) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=35.0->pyopenssl==22.0.0) (2.21)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: cryptography<38 in /usr/local/lib/python3.10/site-packages (37.0.4)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/site-packages (from cryptography<38) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/site-packages (from cffi>=1.12->cryptography<38) (2.21)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting Twisted==22.10.0\n",
      "  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.10/site-packages (from Twisted==22.10.0) (15.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.10/site-packages (from Twisted==22.10.0) (4.8.0)\n",
      "Requirement already satisfied: zope.interface>=4.4.2 in /usr/local/lib/python3.10/site-packages (from Twisted==22.10.0) (6.1)\n",
      "Requirement already satisfied: Automat>=0.8.0 in /usr/local/lib/python3.10/site-packages (from Twisted==22.10.0) (22.10.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/site-packages (from Twisted==22.10.0) (23.1.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.10/site-packages (from Twisted==22.10.0) (21.0.0)\n",
      "Requirement already satisfied: incremental>=21.3.0 in /usr/local/lib/python3.10/site-packages (from Twisted==22.10.0) (22.10.0)\n",
      "Requirement already satisfied: six in /Users/uniba/Library/Python/3.10/lib/python/site-packages (from Automat>=0.8.0->Twisted==22.10.0) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.10/site-packages (from hyperlink>=17.1.1->Twisted==22.10.0) (3.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from zope.interface>=4.4.2->Twisted==22.10.0) (67.6.1)\n",
      "Installing collected packages: Twisted\n",
      "  Attempting uninstall: Twisted\n",
      "    Found existing installation: Twisted 23.8.0\n",
      "    Uninstalling Twisted-23.8.0:\n",
      "      Successfully uninstalled Twisted-23.8.0\n",
      "Successfully installed Twisted-22.10.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyopenssl==22.0.0\n",
    "%pip install \"cryptography<38\"\n",
    "%pip install Twisted==22.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import json\n",
    "import requests\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "#Request URL\n",
    "\n",
    "URL = \"http://127.0.0.1:9201/\"\n",
    "R_URL = URL+\"lercio\"\n",
    "RINS_URL = URL+\"lercio/_doc/\"\n",
    "\n",
    "class LercioScraper(scrapy.Spider):\n",
    "    name = \"LercioPoliticalStories\"\n",
    "    start_urls = [\n",
    "        'https://www.lercio.it/category/politica/',\n",
    "    ]\n",
    "    def parse(self, response):\n",
    "        next_page = response.css('a.next')[0].attrib['href']\n",
    "        next_page_num = int(next_page.split('/')[-2])\n",
    "        if next_page is not None and next_page_num <30:\n",
    "           # print(next_page_num)\n",
    "            yield scrapy.Request(next_page, callback=self.parse, dont_filter=True)\n",
    "            \n",
    "        \n",
    "        #Select the news containers\n",
    "        newses = response.css('.articleTextBox').getall()\n",
    "        for container in enumerate(newses):\n",
    "            #print(container[1])\n",
    "            #Get Title, Date, and Abstract\n",
    "            news = Selector(text=container[1]).xpath('//div/a')\n",
    "            #print(news)\n",
    "            n_link = news[-1].attrib['href']\n",
    "            #print(n_link)\n",
    "            \n",
    "            title = Selector(text=Selector(text=container[1]).css('.articleTitle')[0].get()).xpath('//text()').get().replace(\"\\\"\",\" \").replace(\"'\",\" \")\n",
    "            #print(title)\n",
    "            autore = Selector(text=Selector(text=container[1]).css('.EQarticleAuthor')[0].get()).xpath('//text()')[1].get().replace(\"\\\"\",\" \").replace(\"'\",\" \")\n",
    "            #print(autore)\n",
    "            abstract = Selector(text=Selector(text=container[1]).css('.articleText').get()).xpath('//text()')[0].get().replace(\"\\\"\",\" \").replace(\"'\",\" \")\n",
    "            print(abstract)\n",
    "            \n",
    "            #Create json to send\n",
    "            document = \"{\\\"title\\\":\\\"\"+title+\"\\\", \\\"url\\\":\\\"\"+n_link+\"\\\",\\\"author\\\":\\\"\"+autore+\"\\\",\\\"abstract\\\":\\\"\"+abstract+\"\\\" }\"\n",
    "            JSON_DATA = json.loads(document)\n",
    "            print(JSON_DATA)\n",
    "            response = requests.post(url = RINS_URL, json = JSON_DATA)\n",
    "            response  = response.text \n",
    "\n",
    "            #Convert to JSON\n",
    "            y = json.loads(response)\n",
    "            # pretty printing data\n",
    "            pretty_data = json.dumps(y, indent=4)\n",
    "            #print(pretty_data)\n",
    "           \n",
    "process = CrawlerProcess()\n",
    "process.crawl(LercioScraper)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's do a search\n",
    "#Request URL\n",
    "R_URL = URL+\"lercio/_search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': {'match': {'title': {'query': 'Meloni PNRR', 'operator': 'and'}}}}\n"
     ]
    }
   ],
   "source": [
    "#PAYLOAD\n",
    "R_data =\"{\\\"query\\\": { \\\"match\\\" : { \\\"title\\\" : {\\\"query\\\" : \\\"Meloni PNRR\\\",\\\"operator\\\" : \\\"and\\\"}}} }\"\n",
    "JSON_DATA = y = json.loads(R_data)\n",
    "print(JSON_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-10 17:28:32 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): 127.0.0.1:9201\n",
      "2023-10-10 17:28:32 [urllib3.connectionpool] DEBUG: http://127.0.0.1:9201 \"POST /lercio/_search HTTP/1.1\" 200 None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"took\":6,\"timed_out\":false,\"_shards\":{\"total\":1,\"successful\":1,\"skipped\":0,\"failed\":0},\"hits\":{\"total\":{\"value\":2,\"relation\":\"eq\"},\"max_score\":7.9945927,\"hits\":[{\"_index\":\"lercio\",\"_id\":\"sgswGosB1zxTSTpQsi10\",\"_score\":7.9945927,\"_source\":{\"title\": \"Governo Meloni chiede alla UE di usare i fondi del PNRR per la progettazione del PNRR\", \"url\": \"https://www.lercio.it/governo-meloni-chiede-alla-ue-di-usare-i-fondi-del-pnrr-per-la-progettazione-del-pnrr/\", \"author\": \"Gianni Zoccheddu\", \"abstract\": \" Gianni Zoccheddu\"}},{\"_index\":\"lercio\",\"_id\":\"tAswGosB1zxTSTpQsy2b\",\"_score\":7.4826694,\"_source\":{\"title\": \"PNRR, Meloni:  100 miliardi persi? Colpa dei sessantasette governi precedenti \", \"url\": \"https://www.lercio.it/pnrr-meloni-100-miliardi-persi-colpa-dei-sessantasette-governi-precedenti/\", \"author\": \"Davide Paolino\", \"abstract\": \" Davide Paolino\"}}]}}\n",
      "{\n",
      "    \"took\": 6,\n",
      "    \"timed_out\": false,\n",
      "    \"_shards\": {\n",
      "        \"total\": 1,\n",
      "        \"successful\": 1,\n",
      "        \"skipped\": 0,\n",
      "        \"failed\": 0\n",
      "    },\n",
      "    \"hits\": {\n",
      "        \"total\": {\n",
      "            \"value\": 2,\n",
      "            \"relation\": \"eq\"\n",
      "        },\n",
      "        \"max_score\": 7.9945927,\n",
      "        \"hits\": [\n",
      "            {\n",
      "                \"_index\": \"lercio\",\n",
      "                \"_id\": \"sgswGosB1zxTSTpQsi10\",\n",
      "                \"_score\": 7.9945927,\n",
      "                \"_source\": {\n",
      "                    \"title\": \"Governo Meloni chiede alla UE di usare i fondi del PNRR per la progettazione del PNRR\",\n",
      "                    \"url\": \"https://www.lercio.it/governo-meloni-chiede-alla-ue-di-usare-i-fondi-del-pnrr-per-la-progettazione-del-pnrr/\",\n",
      "                    \"author\": \"Gianni Zoccheddu\",\n",
      "                    \"abstract\": \" Gianni Zoccheddu\"\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"_index\": \"lercio\",\n",
      "                \"_id\": \"tAswGosB1zxTSTpQsy2b\",\n",
      "                \"_score\": 7.4826694,\n",
      "                \"_source\": {\n",
      "                    \"title\": \"PNRR, Meloni:  100 miliardi persi? Colpa dei sessantasette governi precedenti \",\n",
      "                    \"url\": \"https://www.lercio.it/pnrr-meloni-100-miliardi-persi-colpa-dei-sessantasette-governi-precedenti/\",\n",
      "                    \"author\": \"Davide Paolino\",\n",
      "                    \"abstract\": \" Davide Paolino\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(url = R_URL, json = JSON_DATA)\n",
    "response  = response.text\n",
    "print(response)\n",
    "\n",
    "#Convert to JSON\n",
    "y = json.loads(response)\n",
    "# pretty printing data\n",
    "pretty_data = json.dumps(y, indent=4)\n",
    "print(pretty_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
